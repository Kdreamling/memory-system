"""
Memory Gateway - 多后端代理网关
支持 DeepSeek、OpenRouter（GPT-4o、Claude、Gemini等）统一转发
"""

from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import StreamingResponse, JSONResponse
from contextlib import asynccontextmanager
import httpx
import json
import asyncio
from typing import AsyncGenerator, Optional
from datetime import datetime

import sys
sys.path.insert(0, '/home/dream/memory-system/gateway')
from config import get_settings
from services.storage import save_conversation, update_weight
import re
from services.background import sync_service
from routers.mcp_tools import router as mcp_router

settings = get_settings()

# ============ 多后端配置 ============

BACKENDS = {
    # DeepSeek 模型
    "deepseek-chat": {
        "base_url": "https://api.deepseek.com/v1",
        "api_key": settings.llm_api_key,
        "model_name": "deepseek-chat"
    },
    "deepseek-reasoner": {
        "base_url": "https://api.deepseek.com/v1",
        "api_key": settings.llm_api_key,
        "model_name": "deepseek-reasoner"
    },
    
    # OpenAI 模型 (via OpenRouter)
    "gpt-4o": {
        "base_url": "https://openrouter.ai/api/v1",
        "api_key": settings.openrouter_api_key,
        "model_name": "openai/gpt-4o",
        "extra_headers": {"HTTP-Referer": "https://memory-system.local", "X-Title": "Memory Gateway"}
    },
    "gpt-4o-latest": {
        "base_url": "https://openrouter.ai/api/v1",
        "api_key": settings.openrouter_api_key,
        "model_name": "openai/chatgpt-4o-latest",
        "extra_headers": {"HTTP-Referer": "https://memory-system.local", "X-Title": "Memory Gateway"}
    },
    "gpt-4o-2024-11-20": {
        "base_url": "https://openrouter.ai/api/v1",
        "api_key": settings.openrouter_api_key,
        "model_name": "openai/gpt-4o-2024-11-20",
        "extra_headers": {"HTTP-Referer": "https://memory-system.local", "X-Title": "Memory Gateway"}
    },
    
    # Gemini 模型 (via OpenRouter)
    "gemini-3-pro": {
        "base_url": "https://openrouter.ai/api/v1",
        "api_key": settings.openrouter_api_key,
        "model_name": "google/gemini-3-pro-preview",
        "extra_headers": {"HTTP-Referer": "https://memory-system.local", "X-Title": "Memory Gateway"}
    },
    "gemini-3-pro-image": {
        "base_url": "https://openrouter.ai/api/v1",
        "api_key": settings.openrouter_api_key,
        "model_name": "google/gemini-3-pro-image-preview",
        "extra_headers": {"HTTP-Referer": "https://memory-system.local", "X-Title": "Memory Gateway"}
    },
    "gemini-3-flash": {
        "base_url": "https://openrouter.ai/api/v1",
        "api_key": settings.openrouter_api_key,
        "model_name": "google/gemini-3-flash-preview",
        "extra_headers": {"HTTP-Referer": "https://memory-system.local", "X-Title": "Memory Gateway"}
    },
    
    # Claude 模型 (via OpenRouter)
    "claude-sonnet-4.5": {
        "base_url": "https://openrouter.ai/api/v1",
        "api_key": settings.openrouter_api_key,
        "model_name": "anthropic/claude-sonnet-4.5",
        "extra_headers": {"HTTP-Referer": "https://memory-system.local", "X-Title": "Memory Gateway"}
    },
    "claude-opus-4.5": {
        "base_url": "https://openrouter.ai/api/v1",
        "api_key": settings.openrouter_api_key,
        "model_name": "anthropic/claude-opus-4.5",
        "extra_headers": {"HTTP-Referer": "https://memory-system.local", "X-Title": "Memory Gateway"}
    },
}

MODEL_ALIASES = {
    "gpt-4o": "gpt-4o",
    "4o": "gpt-4o",
    "gpt-4o-latest": "gpt-4o-latest",
    "chatgpt-4o-latest": "gpt-4o-latest",
    "gpt-4o-2024-11-20": "gpt-4o-2024-11-20",
    "gemini": "gemini-3-flash",
    "gemini-3-pro": "gemini-3-pro",
    "gemini-3-pro-image": "gemini-3-pro-image",
    "gemini-3-flash": "gemini-3-flash",
    "gemini-pro": "gemini-3-pro",
    "gemini-flash": "gemini-3-flash",
    "claude": "claude-sonnet-4.5",
    "claude-sonnet": "claude-sonnet-4.5",
    "claude-sonnet-4.5": "claude-sonnet-4.5",
    "claude-opus": "claude-opus-4.5",
    "claude-opus-4.5": "claude-opus-4.5",
    "deepseek": "deepseek-chat",
    "deepseek-chat": "deepseek-chat",
    "deepseek-reasoner": "deepseek-reasoner",
}

# ============ 过滤关键词 ============

SYSTEM_KEYWORDS = [
    "<content>", "summarize", "summary", "总结", "标题", "title",
    "I will give you", "system_auto", "health_check",
    "你是一个", "You are a", "As an AI", "作为AI",
    "Generate a concise", "Based on the conversation"
]

def should_skip_storage(user_msg: str) -> bool:
    if not user_msg or len(user_msg.strip()) < 2:
        return True
    for kw in SYSTEM_KEYWORDS:
        if kw.lower() in user_msg.lower():
            return True
    return False

async def process_citations(assistant_msg: str) -> str:
    pattern = r"\[\[used:([a-f0-9-]+)\]\]"
    matches = re.findall(pattern, assistant_msg)
    for conv_id in matches:
        try:
            await update_weight(conv_id)
            print(f"[Citation] Weight updated for {conv_id[:8]}...")
        except Exception as e:
            print(f"[Citation] Error updating weight: {e}")
    clean_msg = re.sub(pattern, "", assistant_msg)
    return clean_msg

def get_backend_config(model: str) -> dict:
    resolved_model = MODEL_ALIASES.get(model.lower(), model)
    if resolved_model in BACKENDS:
        return BACKENDS[resolved_model]
    if "/" in model:
        return {
            "base_url": "https://openrouter.ai/api/v1",
            "api_key": settings.openrouter_api_key,
            "model_name": model,
            "extra_headers": {"HTTP-Referer": "https://memory-system.local", "X-Title": "Memory Gateway"}
        }
    print(f"Unknown model '{model}', falling back to deepseek-chat")
    return BACKENDS["deepseek-chat"]

@asynccontextmanager
async def lifespan(app: FastAPI):
    print("Starting Memory Gateway...")
    print(f"Supported models: {list(BACKENDS.keys())}")
    try:
        await sync_service.start()
    except Exception as e:
        print(f"Warning: Background sync service failed to start: {e}")
    yield
    try:
        await sync_service.stop()
    except:
        pass
    print("Gateway shutdown complete")

app = FastAPI(title="Memory Gateway", lifespan=lifespan)
app.include_router(mcp_router)

@app.get("/health")
async def health_check():
    return {"status": "healthy", "timestamp": datetime.now().isoformat(), "supported_models": list(BACKENDS.keys())}

@app.get("/models")
async def list_models():
    return {"models": list(BACKENDS.keys()), "aliases": MODEL_ALIASES}

@app.post("/v1/chat/completions")
async def proxy_chat_completions(request: Request):
    try:
        body = await request.json()
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Invalid JSON: {e}")
    
    requested_model = body.get("model", "deepseek-chat")
    backend = get_backend_config(requested_model)
    
    if not backend.get("api_key"):
        raise HTTPException(status_code=400, detail=f"API key not configured for model: {requested_model}")
    
    body["model"] = backend["model_name"]
    
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {backend['api_key']}"}
    if "extra_headers" in backend:
        headers.update(backend["extra_headers"])
    
    messages = body.get("messages", [])
    user_msg = ""
    for msg in reversed(messages):
        if msg.get("role") == "user":
            content = msg.get("content", "")
            if isinstance(content, str):
                user_msg = content
            elif isinstance(content, list):
                user_msg = " ".join(part.get("text", "") for part in content if part.get("type") == "text")
            break
    
    is_stream = body.get("stream", False)
    target_url = f"{backend['base_url']}/chat/completions"
    
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {requested_model} -> {backend['model_name']} | stream={is_stream}")
    
    if is_stream:
        return StreamingResponse(stream_and_store(target_url, headers, body, user_msg), media_type="text/event-stream")
    else:
        return await non_stream_request(target_url, headers, body, user_msg)

async def stream_and_store(url: str, headers: dict, body: dict, user_msg: str) -> AsyncGenerator[bytes, None]:
    assistant_chunks = []
    proxy = settings.proxy_url if settings.proxy_url else None
    async with httpx.AsyncClient(timeout=120.0, proxy=proxy) as client:
        async with client.stream("POST", url, headers=headers, json=body) as response:
            if response.status_code != 200:
                error_body = await response.aread()
                yield f"data: {json.dumps({'error': error_body.decode()})}\n\n".encode()
                return
            async for line in response.aiter_lines():
                if line.startswith("data: "):
                    yield f"{line}\n\n".encode()
                    if line == "data: [DONE]":
                        continue
                    try:
                        data = json.loads(line[6:])
                        delta = data.get("choices", [{}])[0].get("delta", {})
                        content = delta.get("content", "")
                        if content:
                            assistant_chunks.append(content)
                    except:
                        pass
    assistant_msg = "".join(assistant_chunks)
    if user_msg and assistant_msg and not should_skip_storage(user_msg):
        try:
            await save_conversation(user_msg, assistant_msg)
        except Exception as e:
            print(f"Storage error: {e}")

async def non_stream_request(url: str, headers: dict, body: dict, user_msg: str) -> JSONResponse:
    proxy = settings.proxy_url if settings.proxy_url else None
    async with httpx.AsyncClient(timeout=120.0, proxy=proxy) as client:
        response = await client.post(url, headers=headers, json=body)
        if response.status_code != 200:
            return JSONResponse(status_code=response.status_code, content={"error": response.text})
        result = response.json()
        assistant_msg = ""
        try:
            assistant_msg = result["choices"][0]["message"]["content"]
        except:
            pass
        if user_msg and assistant_msg and not should_skip_storage(user_msg):
            try:
                await save_conversation(user_msg, assistant_msg)
            except Exception as e:
                print(f"Storage error: {e}")
        return JSONResponse(content=result)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
